#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Sep 25 17:01:29 2017

@author: danny
"""
# this script takes DNN output and labels as created by theano networks and 
# performs several analyses on the output such as calculating the accuracy, precision
# recall, f1 and confusion matrix and saves confusion plots and a log file.
import matplotlib.pyplot as plt
import numpy
import sklearn
from sklearn.metrics import confusion_matrix as confusion
import itertools
import os
from AFvectors import load_predictions, load_targets
import argparse 

parser = argparse.ArgumentParser(description='apply several evaluation metrics and create a confusion matrix')
parser.add_argument('-loc', type = str, default = '/home/danny/Downloads/mlp_predictions',
                    help = 'location of your NN output files')
args = parser.parse_args()



def predictions(loc, pred_files, targ_files, af):
# function to load the network predictions and labels and also return the max predicted
# class label
    # load the model outputs and labels
    pred = load_predictions(loc, [x for x in pred_files if af in x][0])
    targ = load_targets(loc, [x for x in targ_files if af in x][0])
    # get the max predicted label
    max_pred = numpy.argmax(pred, 0)
    
    return pred, targ, max_pred
# function for creating the confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = numpy.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def analysis(pred, targ, max_pred):
    # function which returns several usefull evaluation metrics
   
     # get the overall accuracy
    acc = sum(max_pred == targ)/len(targ)
    # create a confusion matrix
    conf = confusion(targ, max_pred)
    #normalise the confusion matrix
    norm_conf = numpy.transpose(numpy.divide(numpy.transpose(conf),numpy.sum(conf,1)))
    
    # get the number of samples per class, usefull for looking at class imbalance
    samples_class = numpy.sum(conf,1)
    # get the relative size of each class
    weighted_class = samples_class/sum(samples_class)
    
    # f1
    f1 = []
    
    f1.append(sklearn.metrics.f1_score(targ, max_pred ,average = 'weighted'))
    f1.append(sklearn.metrics.f1_score(targ, max_pred ,average = 'micro'))
    f1.append(sklearn.metrics.f1_score(targ, max_pred ,average = 'macro'))
    f1.append(sklearn.metrics.f1_score(targ, max_pred ,average = None))
    # precision
    pr = []
    
    pr.append(sklearn.metrics.precision_score(targ, max_pred ,average = 'weighted'))
    pr.append(sklearn.metrics.precision_score(targ, max_pred ,average = 'micro'))
    pr.append(sklearn.metrics.precision_score(targ, max_pred ,average = 'macro'))
    pr.append(sklearn.metrics.precision_score(targ, max_pred ,average = None))
    #recall
    re = []
    
    re.append(sklearn.metrics.recall_score(targ, max_pred ,average = 'weighted'))
    re.append(sklearn.metrics.recall_score(targ, max_pred ,average = 'micro'))
    re.append(sklearn.metrics.recall_score(targ, max_pred ,average = 'macro'))
    re.append(sklearn.metrics.recall_score(targ, max_pred ,average = None))
    
    
    return acc, conf, norm_conf, weighted_class, f1, pr, re

def main(af, loc):
    # Class names for the labels on the confusion matrix plot
    class_names_place= ['nil', 'bilabial', 'alveolar', 'labiodental', 'velar', 'glottal', 'palatal', 'silence' ]
    class_names_manner= ['vowel', 'plosive','fricative','glide','liquid','nasal','retroflex','silence']
    class_names_voice= ['voiced','unvoiced']
    class_names_backness= ['central', 'back', 'front', 'nil' ]
    class_names_height= ['mid','low','high','nil']
    class_names_round= ['unrounded','rounded','nil']
    class_names_dur=['short','long','diphthong', 'nil']
    
    # dict of AFs
    af_dict = {'place': class_names_place , 'manner': class_names_manner, 'height': class_names_height,
          'voicing': class_names_voice, 'backness': class_names_backness, 'duration': class_names_dur, 'rounding': class_names_round}
    
    files = os.listdir(loc)
    files.sort()
    prediction_files = [f for f in files if 'predictions' in f]
    target_files = [f for f in files if 'targets' in f]

    # get the accuracy, confusion matrix and normalised confusion matrix
    [pred, targ, max_pred] = predictions(loc, prediction_files, target_files, af)
    
    [acc, conf, norm_conf, weighted_class, f1, pr, re] = analysis(pred, targ, max_pred)
    # plot the confusion matrix
    fig = plt.figure()
    plot_confusion_matrix(conf, classes=af_dict[af], normalize=True,
                          title='Feature: ' + af )
    fig.savefig(os.path.join(loc, af + '_conf_matrix.png'))
    plt.close(fig)
    # write all the evaluation metrics to a log file
    with open(os.path.join(loc, 'analysis_log.txt'), 'a') as file:
        file.write('\n articulatory feature: ' + af +'\n')
        file.write('class names in order of appearance: ' + str(af_dict[af]) + '\n')
        file.write('accuracy: ' + str(numpy.round(acc*100,4)) + '% \n')
        file.write('class balance: ' + str(numpy.round(weighted_class*100,4)) + '(%) \n')
        
        file.write('precision (weighted, micro, macro): ' +str(numpy.round(pr[0:-1],4)) + '\n')
        file.write('precision per class: ' +str(numpy.round(pr[-1],4))+ '\n')
        
        file.write('recall (weighted, micro, macro): ' +str(numpy.round(re[0:-1],4)) + '\n')
        file.write('recall per class: ' +str(numpy.round(re[-1],4))+ '\n')
        
        file.write('f1 (weighted, micro, macro): ' +str(numpy.round(f1[0:-1],4)) + '\n')
        file.write('f1 per class: ' +str(numpy.round(f1[-1],4))+ '\n')
    return acc, conf, norm_conf, weighted_class, f1

# location of the predictions and target files
loc = args.loc
af_names = ['manner', 'place', 'voicing', 'height', 'backness', 'duration', 'rounding']
with open(os.path.join(loc, 'analysis_log.txt'), 'w') as file:
    file.write('analysis log for articulatory feature classification' + '\n')    

for af in af_names:
    main(af, loc)